#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Reinforcement Learning Investment Advisor for FiLot Telegram bot
Provides AI-powered investment recommendations using RL techniques
"""

import logging
import time
import random
import json
import os
import math
from typing import Dict, List, Any, Optional, Tuple
import numpy as np

# Import real data clients
from solpool_api_client import get_pools, get_pool_detail
from filotsense_api_client import get_sentiment_simple, get_prices_latest
import agentic_advisor

# Configure logging
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Feature importance weights by risk profile
RISK_PROFILE_WEIGHTS = {
    "conservative": {
        "apr": 0.15,           # Lower weight on APR
        "tvl": 0.30,           # Higher weight on liquidity/stability
        "volume": 0.15,        # Medium weight on volume
        "volatility": -0.20,   # High penalty for volatility
        "sentiment": 0.10,     # Low weight on sentiment
        "prediction": 0.10     # Low weight on AI predictions
    },
    "moderate": {
        "apr": 0.25,           # Medium weight on APR
        "tvl": 0.20,           # Medium weight on liquidity/stability
        "volume": 0.15,        # Medium weight on volume
        "volatility": -0.10,   # Medium penalty for volatility
        "sentiment": 0.15,     # Medium weight on sentiment
        "prediction": 0.15     # Medium weight on AI predictions
    },
    "aggressive": {
        "apr": 0.35,           # High weight on APR
        "tvl": 0.10,           # Low weight on liquidity/stability
        "volume": 0.15,        # Medium weight on volume
        "volatility": -0.05,   # Low penalty for volatility
        "sentiment": 0.15,     # Medium weight on sentiment
        "prediction": 0.20     # High weight on AI predictions
    }
}

# DQN Model configuration
DQN_CONFIG = {
    "learning_rate": 0.001,    # Learning rate for optimizer
    "gamma": 0.95,             # Discount factor for future rewards
    "epsilon": 0.2,            # Exploration rate
    "epsilon_min": 0.01,       # Minimum exploration rate
    "epsilon_decay": 0.995,    # Decay rate for exploration
    "memory_size": 1000,       # Replay memory size
    "batch_size": 32           # Batch size for training
}

# File to store experience replay buffer
EXPERIENCE_BUFFER_FILE = "rl_experience.json"

# Check if PyTorch is available (for actual DQN implementation)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    PYTORCH_AVAILABLE = True
    logger.info("PyTorch is available, using DQN model")
except ImportError:
    PYTORCH_AVAILABLE = False
    logger.warning("PyTorch not available, using simulated RL model")

class SimpleReplayMemory:
    """Simple experience replay memory for RL agent"""
    
    def __init__(self, capacity=1000):
        """Initialize replay memory with given capacity"""
        self.capacity = capacity
        self.memory = []
        self.position = 0
        self._load_memory()
        
    def _load_memory(self):
        """Load experience memory from file if exists"""
        try:
            if os.path.exists(EXPERIENCE_BUFFER_FILE):
                with open(EXPERIENCE_BUFFER_FILE, 'r') as f:
                    self.memory = json.load(f)
                    self.position = len(self.memory) % self.capacity
                    logger.info(f"Loaded {len(self.memory)} experiences from replay memory")
        except Exception as e:
            logger.error(f"Error loading replay memory: {e}")
            self.memory = []
            self.position = 0
            
    def _save_memory(self):
        """Save experience memory to file"""
        try:
            with open(EXPERIENCE_BUFFER_FILE, 'w') as f:
                json.dump(self.memory, f)
        except Exception as e:
            logger.error(f"Error saving replay memory: {e}")
            
    def push(self, state, action, reward, next_state, done):
        """Add experience to memory"""
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = {
            "state": state,
            "action": action,
            "reward": reward,
            "next_state": next_state,
            "done": done
        }
        self.position = (self.position + 1) % self.capacity
        # Periodically save memory
        if self.position % 10 == 0:
            self._save_memory()
            
    def sample(self, batch_size):
        """Sample a batch of experiences from memory"""
        if len(self.memory) < batch_size:
            return []
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        """Return current size of memory"""
        return len(self.memory)

class SimpleDQN:
    """Simple DQN model implementation when PyTorch is not available"""
    
    def __init__(self, state_size, action_size):
        """
        Initialize simulated DQN model
        
        Args:
            state_size: Size of state space (number of features)
            action_size: Size of action space (number of possible actions)
        """
        self.state_size = state_size
        self.action_size = action_size
        self.epsilon = DQN_CONFIG["epsilon"]
        self.epsilon_min = DQN_CONFIG["epsilon_min"]
        self.epsilon_decay = DQN_CONFIG["epsilon_decay"]
        self.memory = SimpleReplayMemory(DQN_CONFIG["memory_size"])
        self.weights = np.random.randn(state_size, action_size) * 0.1
        logger.info(f"Initialized simulated DQN with state size {state_size} and action size {action_size}")
        
    def act(self, state):
        """
        Choose an action based on current state
        
        Args:
            state: Current state vector
            
        Returns:
            Chosen action index
        """
        if np.random.rand() <= self.epsilon:
            # Exploration: random action
            return random.randrange(self.action_size)
        
        # Exploitation: best known action
        act_values = np.dot(state, self.weights)
        return np.argmax(act_values)
    
    def train(self, batch_size=None):
        """
        Simple training step using memory replay
        
        Args:
            batch_size: Batch size for training
        """
        if batch_size is None:
            batch_size = DQN_CONFIG["batch_size"]
            
        if len(self.memory) < batch_size:
            return
        
        # Sample batch from memory
        experiences = self.memory.sample(batch_size)
        
        # Very simple update rule (not a real DQN, just for simulation)
        for experience in experiences:
            state = np.array(experience["state"])
            action = experience["action"]
            reward = experience["reward"]
            next_state = np.array(experience["next_state"])
            done = experience["done"]
            
            # Simple Q-learning update
            target = reward
            if not done:
                target += DQN_CONFIG["gamma"] * np.max(np.dot(next_state, self.weights))
                
            # Update weights for the taken action
            current_q = np.dot(state, self.weights[:, action])
            error = target - current_q
            
            # Update weights for chosen action
            for i in range(self.state_size):
                self.weights[i, action] += DQN_CONFIG["learning_rate"] * error * state[i]
        
        # Decay exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class RLInvestmentAdvisor:
    """RL-based investment advisor that provides optimized pool recommendations"""
    
    def __init__(self):
        """Initialize the RL investment advisor"""
        # State space size: APR, TVL, Volume, Volatility, Sentiment, Prediction, APR Change, Price Change
        self.state_size = 8
        # Action space: top N pools to recommend
        self.action_size = 10
        
        # Initialize DQN model
        if PYTORCH_AVAILABLE:
            # PyTorch DQN implementation would go here
            # For now, just use the simple DQN
            self.model = SimpleDQN(self.state_size, self.action_size)
        else:
            self.model = SimpleDQN(self.state_size, self.action_size)
            
        # For tracking selected investments
        self.investments = {}
            
    async def _extract_features(self, pool: Dict[str, Any], sentiments: Dict[str, Any] = None, prices: Dict[str, Any] = None) -> np.ndarray:
        """
        Extract features from pool data for RL state
        
        Args:
            pool: Pool data dictionary
            sentiments: Optional pre-loaded sentiment data
            prices: Optional pre-loaded price data
            
        Returns:
            Feature vector as numpy array
        """
        # Extract and normalize required values with sensible defaults
        apr = min(pool.get("apr", 0) / 100.0, 1.0)  # Normalize 0-100% to 0-1
        
        # Log scale TVL (liquidity) to deal with wide range
        liquidity = pool.get("liquidity", 0) or 0
        tvl = min(math.log10(liquidity + 1) / 8.0, 1.0)  # log10 scale, max ~$100M
        
        # Log scale volume
        volume_24h = pool.get("volume_24h", 0) or 0
        volume = min(math.log10(volume_24h + 1) / 7.0, 1.0)  # log10 scale, max ~$10M
        
        # Volatility (normalized, higher is more volatile)
        volatility = min(pool.get("volatility", 0) / 100.0, 1.0) if "volatility" in pool else 0.5
        
        # Get token symbols for sentiment and price data
        token1_symbol = pool.get("token1_symbol", "") or ""
        token2_symbol = pool.get("token2_symbol", "") or ""
        
        # Default values if no data available
        token1_sentiment = 0.5  # Default neutral
        token2_sentiment = 0.5  # Default neutral
        price_change_24h = 0.0  # Default no change
        
        # Get sentiment data if available (already in 0-1 scale)
        if sentiments and "sentiment" in sentiments:
            sent_data = sentiments["sentiment"]
            
            if token1_symbol in sent_data:
                score = sent_data[token1_symbol].get("score", 0)
                # Convert from -1:1 to 0:1 scale
                token1_sentiment = (score + 1) / 2
                
            if token2_symbol in sent_data:
                score = sent_data[token2_symbol].get("score", 0)
                # Convert from -1:1 to 0:1 scale
                token2_sentiment = (score + 1) / 2
        
        # Average sentiment for the pool
        sentiment = (token1_sentiment + token2_sentiment) / 2
        
        # Get price change data if available
        if prices and "prices" in prices:
            price_data = prices["prices"]
            
            if token1_symbol in price_data:
                # Normalize price change to 0-1 scale (assume max ±50%)
                percent_change = price_data[token1_symbol].get("percent_change_24h", 0) or 0
                price_change_24h = (percent_change + 50) / 100 if -50 <= percent_change <= 50 else 0.5
        
        # Prediction score (already normalized to 0-1)
        prediction_score = min(pool.get("prediction_score", 50) / 100.0, 1.0)
        
        # APR change (normalized to 0-1 scale, with 0.5 being no change)
        apr_change_7d = pool.get("apr_change_7d", 0) or 0
        # Convert to 0-1 scale with 0.5 as no change, 1.0 as +50% APR, 0.0 as -50% APR
        apr_change_norm = (apr_change_7d + 50) / 100 if -50 <= apr_change_7d <= 50 else 0.5
        
        # Create feature vector
        features = np.array([
            apr,                # APR (0-1)
            tvl,                # TVL (log scaled, 0-1)
            volume,             # Volume (log scaled, 0-1)
            volatility,         # Volatility (0-1)
            sentiment,          # Sentiment (0-1)
            prediction_score,   # Prediction score (0-1)
            apr_change_norm,    # APR change (0-1)
            price_change_24h    # Price change (0-1)
        ])
        
        # Log sample feature values for debugging
        if random.random() < 0.1:  # Only log ~10% of extractions to avoid spam
            logger.debug(f"Pool features: {token1_symbol}/{token2_symbol} - APR: {apr:.2f}, TVL: {tvl:.2f}, " +
                      f"Volume: {volume:.2f}, Volatility: {volatility:.2f}, Sentiment: {sentiment:.2f}, " +
                      f"Prediction: {prediction_score:.2f}, APR Change: {apr_change_norm:.2f}, " +
                      f"Price Change: {price_change_24h:.2f}")
        
        return features
    
    async def _calculate_reward(self, 
                        pool_data: Dict[str, Any], 
                        initial_investment: float,
                        days_held: int,
                        risk_profile: str = "moderate") -> float:
        """
        Calculate reward for RL training based on investment outcome
        
        Args:
            pool_data: Pool data at end of investment period
            initial_investment: Initial investment amount
            days_held: Number of days investment was held
            risk_profile: User's risk profile
            
        Returns:
            Calculated reward value
        """
        # Get feature weights based on risk profile
        if risk_profile not in RISK_PROFILE_WEIGHTS:
            risk_profile = "moderate"
            
        weights = RISK_PROFILE_WEIGHTS[risk_profile]
        
        # Get sentiment and price data
        sentiments = await get_sentiment_simple()
        prices = await get_prices_latest()
        
        # Extract features
        features = await self._extract_features(pool_data, sentiments, prices)
        
        # Calculate weighted feature score (only use the first 6 features that match original weights)
        feature_score = (
            features[0] * weights["apr"] +              # APR
            features[1] * weights["tvl"] +              # TVL
            features[2] * weights["volume"] +           # Volume
            features[3] * weights["volatility"] +       # Volatility
            features[4] * weights["sentiment"] +        # Sentiment
            features[5] * weights["prediction"]         # Prediction
        )
        
        # Calculate estimated return
        apr = pool_data.get("apr", 0)
        estimated_return = initial_investment * (1 + (apr/100) * (days_held/365))
        profit_ratio = (estimated_return - initial_investment) / initial_investment
        
        # Combine feature score with profit ratio
        reward = feature_score * 0.5 + profit_ratio * 0.5
        
        return reward
    
    async def get_recommendations(self,
                           investment_amount: float,
                           risk_profile: str = "moderate",
                           token_preference: Optional[str] = None,
                           max_suggestions: int = 3) -> Dict[str, Any]:
        """
        Get RL-powered investment recommendations
        
        Args:
            investment_amount: Amount to invest (USD)
            risk_profile: User's risk tolerance ("conservative", "moderate", "aggressive")
            token_preference: Optional token to prioritize in recommendations
            max_suggestions: Maximum number of pool suggestions to return
            
        Returns:
            Dictionary with recommendations, explanations, and confidence scores
        """
        logger.info(f"Generating RL-powered recommendations with {risk_profile} profile and ${investment_amount} investment")
        
        # Initialize result
        result = {
            "status": "success",
            "investment_amount": investment_amount,
            "risk_profile": risk_profile,
            "token_preference": token_preference,
            "suggestions": [],
            "market_sentiment": {},
            "explanation": "",
            "fallback_used": False,
            "rl_powered": True,
            "action": "invest"
        }
        
        try:
            # Get candidate pools from real API
            filters = {}
            if token_preference:
                filters["token"] = token_preference
                
            # Fetch pools data
            pools = await get_pools(filters=filters)
            logger.info(f"Fetched {len(pools)} pools from SolPool API")
            
            if not pools or len(pools) < max_suggestions:
                # Fallback to standard agentic advisor
                logger.info("Not enough candidate pools, falling back to standard advisor")
                standard_recommendation = agentic_advisor.get_investment_recommendation(
                    investment_amount, risk_profile, token_preference, max_suggestions
                )
                standard_recommendation["rl_powered"] = False
                return standard_recommendation
            
            # Get real sentiment data
            sentiments = await get_sentiment_simple()
            if not sentiments or not sentiments.get("status") == "success":
                logger.warning("Failed to get sentiment data")
                sentiments = {"sentiment": {}}
            
            # Get real price data
            prices = await get_prices_latest()
            if not prices or not prices.get("status") == "success":
                logger.warning("Failed to get price data")
                prices = {"prices": {}}
            
            # Get market sentiment for result context
            try:
                # Add overall market sentiment (average of BTC and ETH as indicators)
                if "sentiment" in sentiments:
                    sent_data = sentiments["sentiment"]
                    if "BTC" in sent_data and "ETH" in sent_data:
                        btc_score = sent_data["BTC"].get("score", 0)
                        eth_score = sent_data["ETH"].get("score", 0)
                        overall_score = (btc_score + eth_score) / 2
                        result["market_sentiment"]["overall"] = {"score": overall_score}
                    
                    # Add Solana ecosystem sentiment
                    if "SOL" in sent_data:
                        result["market_sentiment"]["solana"] = sent_data["SOL"]
            except Exception as e:
                logger.error(f"Error processing sentiment data: {e}")
            
            # Extract features for each pool
            pool_features = []
            for pool in pools:
                # Skip duplicates by ID
                if any(p["pool_id"] == pool.get("id") for p in pool_features):
                    continue
                
                # Get detailed pool data if needed
                if "volatility" not in pool or "prediction_score" not in pool:
                    try:
                        detailed_pool = await get_pool_detail(pool["id"])
                        if detailed_pool:
                            # Merge detailed data with pool data
                            pool.update(detailed_pool)
                    except Exception as e:
                        logger.error(f"Error getting pool detail: {e}")
                
                # Extract features using real data
                features = await self._extract_features(pool, sentiments, prices)
                
                # Skip invalid pools
                if np.isnan(features).any():
                    continue
                
                # Add to feature list
                pool_features.append({
                    "pool_id": pool.get("id", ""),
                    "pair": f"{pool.get('token1_symbol', '')}/{pool.get('token2_symbol', '')}",
                    "features": features,
                    "raw_data": pool
                })
            
            # Use RL model to rank pools
            ranked_pools = []
            
            # For each candidate pool, calculate Q-value directly
            for pool in pool_features:
                state = pool["features"]
                q_values = np.dot(state, self.model.weights)
                confidence = np.max(q_values) / np.sum(np.abs(q_values)) if np.sum(np.abs(q_values)) > 0 else 0
                
                # Apply risk profile weights to get final score
                weights = RISK_PROFILE_WEIGHTS[risk_profile]
                score = (
                    state[0] * weights["apr"] +        # APR
                    state[1] * weights["tvl"] +        # TVL
                    state[2] * weights["volume"] +     # Volume
                    state[3] * weights["volatility"] + # Volatility
                    state[4] * weights["sentiment"] +  # Sentiment
                    state[5] * weights["prediction"]   # Prediction
                )
                
                # Add token preference bonus
                if token_preference:
                    token1 = pool["raw_data"].get("token1_symbol", "")
                    token2 = pool["raw_data"].get("token2_symbol", "")
                    if token_preference.upper() == token1.upper() or token_preference.upper() == token2.upper():
                        score += 0.1  # 10% bonus for matching token preference
                
                # Build explanation based on feature contributions
                reasons = []
                
                if state[0] > 0.3:  # APR > 30%
                    reasons.append(f"High APR at {pool['raw_data'].get('apr', 0):.1f}%")
                    
                if state[1] > 0.2:  # TVL > $2M
                    reasons.append(f"Strong liquidity depth")
                    
                if state[2] > 0.3:  # Volume > $300K
                    reasons.append(f"High trading volume")
                    
                if state[3] < 0.1:  # Low volatility
                    reasons.append(f"Low price volatility")
                    
                if state[4] > 0.7:  # Positive sentiment
                    reasons.append(f"Positive market sentiment")
                    
                if state[5] > 0.7:  # High prediction score
                    reasons.append(f"Strong prediction confidence")
                    
                if state[6] > 0.7:  # Increasing APR
                    reasons.append(f"APR trending upward")
                    
                if state[7] > 0.7:  # Positive price movement
                    reasons.append(f"Positive price movement")
                
                ranked_pools.append({
                    "pool_id": pool["pool_id"],
                    "pair": pool["pair"],
                    "apr": pool["raw_data"].get("apr", 0),
                    "tvl": pool["raw_data"].get("liquidity", 0),
                    "score": score,
                    "confidence": confidence,
                    "reasons": reasons,
                    "raw_data": pool["raw_data"]
                })
            
            # Sort by score
            ranked_pools.sort(key=lambda x: x["score"], reverse=True)
            
            # Take top suggestions
            top_pools = ranked_pools[:max_suggestions]
            
            # Fill result suggestions
            for pool in top_pools:
                result["suggestions"].append({
                    "pool_id": pool["pool_id"],
                    "pair": pool["pair"],
                    "apr": pool["apr"],
                    "tvl": pool["tvl"],
                    "score": pool["score"],
                    "confidence": pool["confidence"],
                    "reasons": pool["reasons"]
                })
            
            # Generate overall explanation
            if len(result["suggestions"]) > 0:
                # Use market sentiment for general market explanation
                market_conditions = ""
                if "overall" in result["market_sentiment"]:
                    overall_sentiment = result["market_sentiment"]["overall"]["score"]
                    if overall_sentiment > 0.3:
                        market_conditions = "The overall market sentiment is positive, which may support continued growth in liquidity pools."
                    elif overall_sentiment < -0.3:
                        market_conditions = "The overall market sentiment is cautious, suggesting a more conservative approach to pool investments."
                    else:
                        market_conditions = "The market sentiment is neutral, with balanced opportunities for strategic pool investments."
                
                # Add Solana-specific explanation if available
                solana_conditions = ""
                if "solana" in result["market_sentiment"]:
                    solana_sentiment = result["market_sentiment"]["solana"]["score"]
                    if solana_sentiment > 0.3:
                        solana_conditions = " Solana ecosystem sentiment is positive, creating favorable conditions for SOL-based pools."
                    elif solana_sentiment < -0.3:
                        solana_conditions = " Solana ecosystem sentiment shows some caution, consider diversifying across different token pairs."
                
                # Get top recommendation details
                top_pool = result["suggestions"][0]
                confidence_level = "high" if top_pool["confidence"] > 0.7 else "moderate" if top_pool["confidence"] > 0.4 else "tentative"
                top_reasons = ", ".join(top_pool["reasons"])
                
                # Combine into comprehensive explanation
                result["explanation"] = f"{market_conditions}{solana_conditions} Based on AI analysis with {confidence_level} confidence, "
                result["explanation"] += f"the {top_pool['pair']} pool at {top_pool['apr']:.1f}% APR offers the best opportunity because: {top_reasons}."
            
            return result
            
        except Exception as e:
            logger.error(f"Error in RL investment advisor: {e}")
            # Fallback to standard agentic advisor
            standard_recommendation = agentic_advisor.get_investment_recommendation(
                investment_amount, risk_profile, token_preference, max_suggestions
            )
            standard_recommendation["rl_powered"] = False
            standard_recommendation["fallback_used"] = True
            return standard_recommendation
    
    async def record_investment(self, user_id: int, pool_id: str, investment_amount: float, risk_profile: str):
        """
        Record an investment for RL training feedback
        
        Args:
            user_id: User ID making the investment
            pool_id: Pool ID that was invested in
            investment_amount: Amount invested
            risk_profile: User's risk profile
        """
        try:
            # Get pool details from real API
            pool_details = await get_pool_detail(pool_id)
            
            if not pool_details:
                logger.error(f"Could not get pool details for {pool_id}")
                return
            
            # Get sentiment and price data
            sentiments = await get_sentiment_simple()
            prices = await get_prices_latest()
            
            # Record initial state
            initial_state = await self._extract_features(pool_details, sentiments, prices)
            
            # Store investment record
            investment_id = f"{user_id}_{pool_id}_{int(time.time())}"
            self.investments[investment_id] = {
                "user_id": user_id,
                "pool_id": pool_id,
                "investment_amount": investment_amount,
                "risk_profile": risk_profile,
                "start_time": time.time(),
                "initial_state": initial_state.tolist(),
                "initial_apr": pool_details.get("apr", 0),
                "initial_tvl": pool_details.get("liquidity", 0)
            }
            
            logger.info(f"Recorded investment {investment_id} for RL training")
            
        except Exception as e:
            logger.error(f"Error recording investment: {e}")
    
    async def feedback_investment(self, user_id: int, pool_id: str, rating: int, exit_amount: Optional[float] = None):
        """
        Provide feedback on a previous investment for RL training
        
        Args:
            user_id: User ID who made the investment
            pool_id: Pool ID that was invested in
            rating: User rating (1-5) of how good the investment was
            exit_amount: Optional amount received upon exit
        """
        try:
            # Find matching investment
            found = False
            for investment_id, investment in self.investments.items():
                if investment["user_id"] == user_id and investment["pool_id"] == pool_id:
                    found = True
                    
                    # Get current pool details from real API
                    pool_details = await get_pool_detail(pool_id)
                    
                    if not pool_details:
                        logger.error(f"Could not get pool details for {pool_id}")
                        return
                    
                    # Calculate days held
                    days_held = (time.time() - investment["start_time"]) / (60 * 60 * 24)
                    
                    # Get sentiment and price data
                    sentiments = await get_sentiment_simple()
                    prices = await get_prices_latest()
                    
                    # Extract features for current state
                    current_state = await self._extract_features(pool_details, sentiments, prices)
                    
                    # Calculate reward
                    # If exit_amount provided, use actual return
                    if exit_amount:
                        profit_ratio = (exit_amount - investment["investment_amount"]) / investment["investment_amount"]
                        # Scale profit ratio to a reward (-1 to 1)
                        base_reward = min(max(profit_ratio * 5, -1), 1)
                    else:
                        # Use estimated return from APR
                        base_reward = await self._calculate_reward(
                            pool_details, 
                            investment["investment_amount"],
                            days_held,
                            investment["risk_profile"]
                        )
                    
                    # Incorporate user rating (1-5) into reward
                    user_reward = (rating - 3) / 2  # Scale 1-5 to -1 to 1
                    
                    # Combine base reward with user feedback (70% base, 30% user feedback)
                    reward = base_reward * 0.7 + user_reward * 0.3
                    
                    # Add experience to replay memory
                    self.model.memory.push(
                        investment["initial_state"],  # Initial state
                        0,                           # Action (arbitrary for feedback)
                        reward,                      # Reward
                        current_state.tolist(),      # Final state
                        True                         # Terminal state
                    )
                    
                    # Train model on this experience
                    self.model.train(batch_size=1)
                    
                    logger.info(f"Added feedback for investment {investment_id} with reward {reward:.2f}")
                    
                    # Remove from active investments
                    self.investments.pop(investment_id)
                    break
            
            if not found:
                logger.warning(f"No matching investment found for user {user_id} and pool {pool_id}")
                
        except Exception as e:
            logger.error(f"Error processing investment feedback: {e}")
            
# Create a singleton instance
rl_advisor = RLInvestmentAdvisor()

async def get_rl_recommendations(
    investment_amount: float,
    risk_profile: str = "moderate",
    token_preference: Optional[str] = None,
    max_suggestions: int = 3
) -> Dict[str, Any]:
    """
    Get smart investment recommendation using RL
    
    Args:
        investment_amount: Amount to invest (USD)
        risk_profile: User's risk tolerance ("conservative", "moderate", "aggressive")
        token_preference: Optional token to prioritize in recommendations
        max_suggestions: Maximum number of pool suggestions to return
        
    Returns:
        Dictionary with recommendations, explanations, and data sources
    """
    global rl_advisor
    return await rl_advisor.get_recommendations(
        investment_amount, risk_profile, token_preference, max_suggestions
    )

async def record_smart_investment(user_id: int, pool_id: str, investment_amount: float, risk_profile: str):
    """
    Record a smart investment for RL training
    
    Args:
        user_id: User ID making the investment
        pool_id: Pool ID that was invested in 
        investment_amount: Amount invested
        risk_profile: User's risk profile
    """
    global rl_advisor
    await rl_advisor.record_investment(user_id, pool_id, investment_amount, risk_profile)
    
async def feedback_smart_investment(user_id: int, pool_id: str, rating: int, exit_amount: Optional[float] = None):
    """
    Provide feedback on a smart investment
    
    Args:
        user_id: User ID who made the investment
        pool_id: Pool ID that was invested in
        rating: User rating (1-5) of how good the investment was
        exit_amount: Optional amount received upon exit
    """
    global rl_advisor
    await rl_advisor.feedback_investment(user_id, pool_id, rating, exit_amount)

# Simple test function
async def test_rl_recommendations():
    """Test the RL recommendations with real data"""
    print("Testing RL investment recommendations with real data")
    
    # Test parameters
    investment_amount = 1000.0
    risk_profiles = ["conservative", "moderate", "aggressive"]
    
    for risk_profile in risk_profiles:
        print(f"\n--- Testing {risk_profile.upper()} risk profile ---")
        
        # Get recommendations
        recommendations = await get_rl_recommendations(
            investment_amount=investment_amount,
            risk_profile=risk_profile,
            max_suggestions=3
        )
        
        # Check if recommendations were successful
        if recommendations.get("status") == "success":
            print(f"RL Powered: {recommendations.get('rl_powered', False)}")
            print(f"Fallback Used: {recommendations.get('fallback_used', False)}")
            
            # Print suggestions
            suggestions = recommendations.get("suggestions", [])
            if suggestions:
                print(f"\nTop {len(suggestions)} recommended pools:")
                for i, suggestion in enumerate(suggestions):
                    print(f"{i+1}. {suggestion.get('pair')} - APR: {suggestion.get('apr', 0):.2f}% - Score: {suggestion.get('score', 0):.4f}")
                    print(f"   Reasons: {', '.join(suggestion.get('reasons', []))}")
            else:
                print("No pool suggestions found.")
                
            # Print explanation
            explanation = recommendations.get("explanation", "")
            if explanation:
                print(f"\nExplanation: {explanation}")
        else:
            print(f"Error getting recommendations: {recommendations.get('error', 'Unknown error')}")
    
    print("\nTest completed!")

# Run test if executed directly
if __name__ == "__main__":
    import asyncio
    asyncio.run(test_rl_recommendations())